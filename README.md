# AI System Learning Path | AI系统研发学习路径

> 北京理工大学软件工程硕士 | 聚焦AI系统/软件研发（高性能计算、模型推理与部署优化）

**目标岗位**：AI系统/软件研发工程师、高性能计算工程师、机器学习框架研发工程师  

这是一个记录我向AI系统研发工程师目标迈进的学习与实践仓库。我通过**从底层硬件到上层服务**的完整项目实践，构建扎实的工程能力，目标在于深入理解AI基础设施的核心原理，并掌握工业级部署与优化的全流程技能。

## 🗺️ 学习与实践路径

### 阶段一：内功筑基 - 深入框架与CUDA编程

- **目标**：从框架使用者转变为理解内部机制，并能进行底层性能优化的开发者。
- **核心项目**：[01_cuda_swish_extension](/projects/01_cuda_swish_extension/)：为PyTorch实现了一个高性能的Swish激活函数**CUDA算子**。从零实现了前向与反向传播内核，并进行了性能分析与优化（如算子融合）。通过此项目，我深入理解了GPU并行计算模型、内存层次结构以及PyTorch的C++扩展机制。
- **关键技术点**：CUDA C++、PyTorch C++ Extension、GPU性能剖析（Nsight Systems）、自动求导。

### 阶段二：系统实战 - 从零构建微型深度学习框架

- **目标**：系统性理解深度学习框架的全貌，掌握计算图与自动求导的核心实现。
- **核心项目**：[02_tiny_dl_framework](/projects/02_tiny_dl_framework/)：仅使用**C++/CUDA**，从零实现了一个轻量级深度学习框架。核心特性包括：
  - 支持CPU/GPU的`Tensor`类与内存管理。
  - 基于计算图的**自动求导引擎**。
  - 卷积、池化等基础算子的实现。
  - 优化器（SGD）与简单的`nn.Module`封装。
  - 在MNIST数据集上成功训练了一个CNN模型进行验证。
- **关键技术点**：计算图、链式法则、设计模式、CMake构建。

### 阶段三：工业实践 - 端到端模型推理优化与部署

- **目标**：聚焦工业界核心的模型部署问题，打造从算法到服务的完整工程闭环。
- **核心项目**：[03_model_deployment_pipeline](/projects/03_model_deployment_pipeline/)：构建一个完整的**模型压缩-推理-服务化**Pipeline。
  1. **模型压缩**：对预训练的ResNet-18模型进行**剪枝（Pruning）** 与 **INT8量化（Quantization）**，显著减少模型体积与计算量。
  2. **推理引擎开发**：使用纯**C++** 实现一个极简的神经网络推理引擎，支持加载量化后的模型，并进行高效的前向计算。
  3. **服务化部署**：将推理引擎封装为**高性能HTTP REST API**服务，并使用**Docker**进行容器化封装，实现一键部署。
  4. **性能基准测试**：系统化对比原始模型与优化后模型在精度、速度、内存占用上的差异，并形成报告。
- **关键技术点**：模型压缩（PyTorch）、C++高性能编程、网络编程（如gRPC/ libhv）、Docker容器化。

### 阶段四：前沿探索 - 硬件抽象与未来编程模型

- **目标**：超越单一硬件生态，探索下一代AI编程模型，构建技术前瞻性。
- **探索项目**：[04_triton_exploration](/projects/04_triton_exploration/)：使用**OpenAI Triton**语言重写了第一阶段的自定义算子。通过对比**纯PyTorch**、**原生CUDA**、**Triton**三种实现的性能与代码复杂度，我深入理解了高层抽象与底层硬件之间的权衡，为适配多样化的AI加速器奠定了基础。

## 🛠️ 技术栈全景

| 类别        | 具体技术                                         |
|:--------- |:-------------------------------------------- |
| **编程语言**  | C++ (核心)、CUDA、Python                         |
| **深度学习**  | PyTorch（框架与内部机制）、模型压缩（Pruning, Quantization） |
| **系统与性能** | 计算机体系结构、操作系统、高性能计算、性能剖析（Nsight）              |
| **工程与部署** | Git、CMake、Docker、HTTP/REST API 开发            |
| **前沿探索**  | OpenAI Triton、模型编译与部署生态                      |

## 📂 仓库结构

```
AI-System-Learning/
├── projects/               # 所有项目代码
│   ├── 01_cuda_swish_extension/
│   ├── 02_tiny_dl_framework/
│   ├── 03_model_deployment_pipeline/  # 第三阶段核心项目
│   └── 04_triton_exploration/
├── notes/                  # 学习笔记与知识总结
├── resources/              # 有用的参考资料
└── README.md              # 本文件
```

## 📈 通过这些项目我证明了什么？

1. **扎实的底层功底**：通过手写CUDA算子和微型框架，我深刻理解AI计算的底层硬件原理和软件栈基础，而非仅停留在API调用层面。
2. **解决复杂工程问题的能力**：第三阶段项目展示了我将学术模型转化为稳定、高效、可扩展的生产服务的**全流程能力**，这是企业级AI研发的核心。
3. **持续学习与前沿视野**：我对Triton等新技术的探索，表明我具备跟踪技术趋势并快速学习的能力，能适应快速演进的AI基础设施领域。

## 📬 联系与交流

- **GitHub**: [@YY4994](https://github.com/YY4994) - 本项目所有代码均在此更新。
- 我正积极寻求**2026年暑期AI系统研发方向的实习机会**。如果我的项目经历与贵团队的技术方向契合，欢迎通过GitHub Issues或邮件联系我。

---

> *学习的过程是“知道它是什么”，而实践的目标是“弄清楚它为什么是这样，并让它变得更好”。*
