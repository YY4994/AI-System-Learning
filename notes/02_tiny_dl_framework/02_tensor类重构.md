# Tensorç±»é‡æ„æ–¹æ¡ˆï¼šå¼•å…¥æ˜¾å¼TensorImpl

## ğŸ¯ é‡æ„çš„æ ¸å¿ƒç›®æ ‡ä¸åŸç†

### **ä¸ºä»€ä¹ˆå¿…é¡»å¼•å…¥æ˜¾å¼TensorImplï¼Ÿ**

å½“å‰èåˆè®¾è®¡çš„æ ¹æœ¬çŸ›ç›¾ï¼š**TensoråŒæ—¶æ‰¿æ‹…äº†å¤ªå¤šèŒè´£**ï¼Œå¯¼è‡´è‡ªåŠ¨æ±‚å¯¼æ—¶å‡ºç°**æ— æ³•è§£å†³çš„è§’è‰²å†²çª**ã€‚

**å…·ä½“å†²çª**ï¼š

1. **èº«ä»½å†²çª**ï¼šTensoræ—¢æ˜¯â€œæ•°æ®å®¹å™¨â€åˆæ˜¯â€œè®¡ç®—å›¾èŠ‚ç‚¹â€
2. **æ‰€æœ‰æƒå¾ªç¯é£é™©**ï¼š`Tensor A` æŒæœ‰ `Tensor B` ä½œä¸ºæ¢¯åº¦ï¼Œ`Tensor B` åˆå¯èƒ½å¼•ç”¨å…¶ä»–Function...
3. **ç‰ˆæœ¬æ§åˆ¶ç¼ºå¤±**ï¼šæ— æ³•æ£€æµ‹in-placeæ“ä½œï¼ˆå¦‚ `x.relu_()` ä¼šä¿®æ”¹åŸæ•°æ®ï¼Œç ´ååå‘ä¼ æ’­ï¼‰

### **æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼šå•ä¸€èŒè´£åˆ†ç¦»**

```
é‡æ„å‰ï¼š
Tensor = æ•°æ®å­˜å‚¨ + è§†å›¾å‚æ•° + è‡ªåŠ¨æ±‚å¯¼å…ƒæ•°æ® + ç”¨æˆ·æ¥å£

é‡æ„åï¼š
Tensor (ç”¨æˆ·å¥æŸ„) â†’ ä»…æä¾›ç”¨æˆ·æ¥å£
    â†“ æŒæœ‰
TensorImpl (å®ç°å±‚) â†’ æ•°æ® + è§†å›¾ + è‡ªåŠ¨æ±‚å¯¼çŠ¶æ€
    â†“ æŒæœ‰  
Storage (çº¯æ•°æ®å—) â†’ åŸå§‹å†…å­˜
```

## ğŸ—ï¸ æ–°æ¶æ„è¯¦ç»†è®¾è®¡

### **1. Storageå±‚ï¼ˆä¸å˜ï¼Œä¿æŒç°æœ‰ï¼‰**

```cpp
template <typename Scalar, typename Allocator>
class Storage {
    // ä¿æŒä½ ç°æœ‰çš„è®¾è®¡ï¼Œçº¯æ•°æ®å®¹å™¨
    Scalar* data_;
    size_t capacity_;
    Allocator allocator_;
    // ... åˆ†é…/é‡Šæ”¾æ–¹æ³•
};
```

### **2. TensorImplå±‚ï¼ˆæ–°å¢ï¼Œæ ¸å¿ƒå®ç°å±‚ï¼‰**

è¿™æ˜¯**è‡ªåŠ¨æ±‚å¯¼ç³»ç»Ÿçš„æ ¸å¿ƒæ‰¿è½½è€…**ï¼Œæ¯ä¸ªå”¯ä¸€çš„ï¼ˆæ•°æ®+è§†å›¾+æ¢¯åº¦çŠ¶æ€ï¼‰å¯¹åº”ä¸€ä¸ªTensorImplå®ä¾‹ã€‚

```cpp
template <typename Scalar, typename Device>
class TensorImpl {
private:
    // === ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®ä¸è§†å›¾ï¼ˆä¸å¯å˜æ ¸å¿ƒï¼‰===
    std::shared_ptr<Storage<Scalar>> storage_;  // æ•°æ®æ‰€æœ‰æƒ
    std::vector<size_t> shape_;                 // å½¢çŠ¶ï¼ˆåˆ›å»ºååº”ä¸å˜ï¼‰
    std::vector<size_t> strides_;               // æ­¥é•¿ï¼ˆåˆ›å»ºååº”ä¸å˜ï¼‰
    size_t offset_ = 0;                         // å­˜å‚¨ä¸­çš„åç§»

    // === ç¬¬äºŒéƒ¨åˆ†ï¼šè‡ªåŠ¨æ±‚å¯¼çŠ¶æ€ï¼ˆå¯å˜ï¼‰===
    std::weak_ptr<Function> grad_fn_;           // å…³é”®ï¼šå¼±å¼•ç”¨åˆ°åˆ›å»ºè€…Function
    std::shared_ptr<TensorImpl> grad_;          // æ¢¯åº¦æœ¬èº«æ˜¯å¦ä¸€ä¸ªTensorImpl
    bool requires_grad_ = false;

    // === ç¬¬ä¸‰éƒ¨åˆ†ï¼šç‰ˆæœ¬ä¸å…ƒæ•°æ® ===
    size_t version_ = 0;                        // ç”¨äºæ£€æµ‹in-placeä¿®æ”¹
    size_t unique_id_;                          // å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºè°ƒè¯•

public:
    // æ„é€ å‡½æ•°ï¼šä»æ•°æ®åˆ›å»º
    TensorImpl(std::shared_ptr<Storage<Scalar>> storage, 
               std::vector<size_t> shape,
               std::vector<size_t> strides,
               size_t offset);

    // æ„é€ å‡½æ•°ï¼šåˆ›å»ºè§†å›¾ï¼ˆå…±äº«æ•°æ®ï¼‰
    TensorImpl(std::shared_ptr<TensorImpl> other,  // å…±äº«storage_
               std::vector<size_t> new_shape,
               std::vector<size_t> new_strides,
               size_t new_offset);

    // === å…³é”®æ–¹æ³• ===

    // æ•°æ®è®¿é—®
    Scalar* data() { return storage_->data() + offset_; }

    // è‡ªåŠ¨æ±‚å¯¼ç›¸å…³
    void set_gradient(std::shared_ptr<TensorImpl> grad) { 
        grad_ = grad; 
    }

    void set_grad_fn(std::shared_ptr<Function> fn) { 
        grad_fn_ = fn;  // å¼±å¼•ç”¨ï¼ä¸å¢åŠ Functionå¼•ç”¨è®¡æ•°
    }

    void mark_modified() { version_++; }  // in-placeæ“ä½œæ—¶è°ƒç”¨

    // æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ•ˆï¼ˆç”¨äºFunctionçš„backwardï¼‰
    bool is_valid_input(size_t saved_version) const {
        return version_ == saved_version;  // ç‰ˆæœ¬å·åŒ¹é…è¯´æ˜æœªè¢«ä¿®æ”¹
    }

    // åˆ›å»ºè§†å›¾ï¼ˆå·¥å‚æ–¹æ³•ï¼‰
    std::shared_ptr<TensorImpl> view(std::vector<size_t> new_shape);
};
```

### **3. Tensorå±‚ï¼ˆç”¨æˆ·å¥æŸ„ï¼Œè½»é‡çº§ï¼‰**

è¿™æ˜¯**ç”¨æˆ·å”¯ä¸€ç›´æ¥æ¥è§¦çš„ç±»**ï¼Œéå¸¸è½»é‡ï¼Œæ‹·è´æˆæœ¬ä½ã€‚

```cpp
template <typename Scalar, typename Device>
class Tensor {
private:
    // å”¯ä¸€çš„æ ¸å¿ƒæ•°æ®æˆå‘˜ï¼šæŒ‡å‘å®ç°çš„å…±äº«æŒ‡é’ˆ
    std::shared_ptr<TensorImpl<Scalar, Device>> impl_;

public:
    using ImplType = TensorImpl<Scalar, Device>;

    // === æ„é€ å‡½æ•° ===

    // 1. ä»ç°æœ‰å®ç°åˆ›å»º
    explicit Tensor(std::shared_ptr<ImplType> impl) : impl_(impl) {}

    // 2. ä»å½¢çŠ¶åˆ›å»ºï¼ˆåˆ†é…æ–°å­˜å‚¨ï¼‰
    explicit Tensor(std::vector<size_t> shape) {
        size_t total_size = compute_total_size(shape);
        auto storage = std::make_shared<Storage<Scalar>>(total_size);
        auto strides = compute_strides(shape);
        impl_ = std::make_shared<ImplType>(storage, shape, strides, 0);
    }

    // 3. ä»æ ‡é‡å€¼åˆ›å»ºï¼ˆå¹¿æ’­ï¼‰
    Tensor(Scalar value, std::vector<size_t> shape) {
        *this = Tensor(shape);
        std::fill(data(), data() + size(), value);
    }

    // === æ•°æ®è®¿é—®ï¼ˆå§”æ‰˜ç»™impl_ï¼‰===

    Scalar* data() { return impl_->data(); }
    const std::vector<size_t>& shape() const { return impl_->shape(); }
    size_t size() const { return impl_->size(); }

    // ç´¢å¼•è®¿é—®ï¼ˆå…³é”®ï¼šè¿”å›æ–°Tensorå¥æŸ„ï¼Œè€Œä¸æ˜¯å¼•ç”¨ï¼‰
    Tensor operator()(std::initializer_list<size_t> indices) {
        // è®¡ç®—å…ƒç´ åç§»
        size_t elem_index = impl_->compute_offset(indices);

        // åˆ›å»ºæ ‡é‡Tensorçš„è§†å›¾ï¼ˆå½¢çŠ¶[1]ï¼Œåç§»åˆ°å…·ä½“å…ƒç´ ï¼‰
        // æ³¨æ„ï¼šè¿™æ˜¯è§†å›¾ï¼Œå…±äº«æ•°æ®
        return Tensor(impl_->view_single_element(elem_index));
    }

    // === è‡ªåŠ¨æ±‚å¯¼æ¥å£ ===

    Tensor grad() const {
        if (!impl_->grad()) {
            return Tensor();  // è¿”å›ç©ºTensor
        }
        return Tensor(impl_->grad());  // åŒ…è£…æˆTensorå¥æŸ„
    }

    void set_grad(const Tensor& grad) {
        // é‡è¦ï¼šgrad.impl_ æ˜¯ gradçš„TensorImpl
        impl_->set_gradient(grad.impl_);
    }

    bool requires_grad() const { return impl_->requires_grad(); }
    void requires_grad(bool requires) { impl_->set_requires_grad(requires); }

    // === è§†å›¾æ“ä½œï¼ˆè¿”å›æ–°Tensorå¥æŸ„ï¼‰===

    Tensor view(std::vector<size_t> new_shape) {
        return Tensor(impl_->view(new_shape));
    }

    Tensor reshape(std::vector<size_t> new_shape) {
        // éªŒè¯å¤§å°åŒ¹é…ï¼Œç„¶ååˆ›å»ºè§†å›¾
        return view(new_shape);
    }

    // === è¿ç®—ç¬¦é‡è½½ï¼ˆç¤ºä¾‹ï¼šåŠ æ³•ï¼‰===

    Tensor operator+(const Tensor& other) const {
        // 1. æ‰§è¡Œå®é™…è®¡ç®—ï¼ˆåˆ†é…æ–°å­˜å‚¨ï¼‰
        Tensor result(this->shape());
        elementwise_add(this->data(), other.data(), result.data(), size());

        // 2. å¦‚æœéœ€è¦æ¢¯åº¦ï¼Œåˆ›å»ºFunctionè®°å½•è®¡ç®—
        if (this->requires_grad() || other.requires_grad()) {
            auto add_fn = std::make_shared<AddFunction>();

            // å…³é”®ï¼šè®¾ç½®æ–°Tensorçš„grad_fn
            result.impl_->set_grad_fn(add_fn);

            // Functionéœ€è¦è®°å½•è¾“å…¥ä¿¡æ¯
            add_fn->save_inputs({
                InputInfo{this->impl_, this->impl_->version()},
                InputInfo{other.impl_, other.impl_->version()}
            });

            // è®¾ç½®è¾“å‡º
            add_fn->set_output(result.impl_);

            // æ–°Tensoréœ€è¦æ¢¯åº¦
            result.requires_grad(true);
        }

        return result;
    }

    // === åå‘ä¼ æ’­å…¥å£ ===

    void backward(Tensor grad = Tensor()) {
        if (!grad.impl_) {
            // åˆ›å»ºå…¨1çš„æ¢¯åº¦ï¼Œå½¢çŠ¶ä¸thisç›¸åŒ
            grad = Tensor(1.0, this->shape());
        }

        // å§”æ‰˜ç»™impl_çš„åå‘ä¼ æ’­å¼•æ“
        AutogradEngine::backward(impl_, grad.impl_);
    }
};
```

## ğŸ”— å…³é”®å…³ç³»ä¸å†…å­˜ç®¡ç†

### **1. æ‰€æœ‰æƒå…³ç³»ï¼ˆæ— å¾ªç¯å¼•ç”¨ï¼‰**

```
Tensor A â”€â”€æŒæœ‰â”€â”€> TensorImpl_A â”€â”€æŒæœ‰â”€â”€> Storage
    â†‘                      â†“
    |                weak_ptr<Function>ï¼ˆä¸å¢åŠ è®¡æ•°ï¼‰
    |                      â†“
Tensor grad_A â†â”€æŒæœ‰â”€â”€ TensorImpl_grad_A
```

### **2. ç‰ˆæœ¬æ§åˆ¶çš„å·¥ä½œæµç¨‹**

```cpp
// in-placeæ“ä½œç¤ºä¾‹ï¼šReLUåŸåœ°æ¿€æ´»
void relu_(Tensor& input) {
    // ä¿®æ”¹æ•°æ®
    for (size_t i = 0; i < input.size(); ++i) {
        if (input.data()[i] < 0) input.data()[i] = 0;
    }

    // å…³é”®ï¼šæ ‡è®°å·²ä¿®æ”¹
    input.impl_->mark_modified();  // version_++
}

// åå‘ä¼ æ’­æ—¶æ£€æµ‹
bool Function::check_inputs() {
    for (auto& input_info : saved_inputs_) {
        // æå‡å¼±å¼•ç”¨ä¸ºshared_ptr
        auto impl = input_info.impl_weak.lock();

        if (!impl) {
            // TensorImplå·²è¢«é‡Šæ”¾ï¼Œè®¡ç®—å›¾æ— æ•ˆ
            return false;
        }

        if (impl->version() != input_info.saved_version) {
            // ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œè¾“å…¥è¢«in-placeä¿®æ”¹è¿‡
            throw std::runtime_error("One of the inputs has been modified in-place");
        }
    }
    return true;
}
```

### **3. æ¢¯åº¦ç´¯åŠ æ¨¡å¼**

```cpp
// å½“å¤šä¸ªFunctionè´¡çŒ®ç»™åŒä¸€ä¸ªTensorçš„æ¢¯åº¦æ—¶ï¼š
// TensorImplå†…éƒ¨ï¼š
void accumulate_gradient(std::shared_ptr<TensorImpl> new_grad) {
    if (!grad_) {
        grad_ = new_grad;  // ç¬¬ä¸€æ¬¡ï¼Œç›´æ¥èµ‹å€¼
    } else {
        // åç»­ï¼Œç´¯åŠ ï¼šgrad_ = grad_ + new_grad
        // éœ€è¦åˆ›å»ºæ–°çš„TensorImplæ¥å­˜å‚¨ç´¯åŠ ç»“æœ
        grad_ = elementwise_add(grad_, new_grad);
    }
}
```

## ğŸš€ é‡æ„çš„ä¼˜åŠ¿

1. **è§£å†³å¾ªç¯å¼•ç”¨**ï¼šTensorå¥æŸ„è½»é‡ï¼ŒTensorImplæœ‰æ¸…æ™°çš„æ‰€æœ‰æƒé“¾
2. **ç‰ˆæœ¬æ§åˆ¶**ï¼šæ”¯æŒin-placeæ“ä½œæ£€æµ‹
3. **è°ƒè¯•å‹å¥½**ï¼šæ¯ä¸ªTensorImplæœ‰å”¯ä¸€IDï¼Œä¾¿äºè·Ÿè¸ªè®¡ç®—å›¾
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šå¯åœ¨TensorImplå±‚åšæ¢¯åº¦ç´¯åŠ ç­‰ä¼˜åŒ–ï¼Œå¯¹ç”¨æˆ·é€æ˜
5. **æ‰©å±•æ€§**ï¼šæ˜“äºæ·»åŠ æ–°ç‰¹æ€§ï¼ˆå¦‚è®¾å¤‡è¿ç§»ã€åºåˆ—åŒ–ï¼‰

# 
