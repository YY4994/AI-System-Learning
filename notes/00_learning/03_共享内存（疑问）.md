# CUDA内核执行模型详解

## 一、CUDA执行模型核心概念

### 1.1 `__global__`函数：每个线程都执行

```cpp
// 关键理解：当你启动内核时
matrixMulShared<<<gridDim, blockDim>>>(A, B, C, N);

// 这会创建 gridDim.x × gridDim.y × gridDim.z 个线程块
// 每个线程块有 blockDim.x × blockDim.y × blockDim.z 个线程
// 总共：总线程数 = gridDim × blockDim
```

**每个线程都独立执行这个`__global__`函数**，就好像每个线程都有自己的一份代码副本在执行。

## 二、共享内存的协作加载详解

### 2.1 问题1：每个线程都要往共享内存中加载吗？

**答案是：是的，每个线程都执行加载代码，但加载的是不同的数据元素**

```cpp
// 看这段代码：
if (A_row_index < N && A_col_index < N) {
    sharedA[ty][tx] = A[A_row_index * N + A_col_index];
}

// 关键：sharedA[ty][tx] 中的 ty 和 tx 是线程索引
// 这意味着：
// 线程(0,0) 写入 sharedA[0][0]
// 线程(0,1) 写入 sharedA[0][1]
// 线程(1,0) 写入 sharedA[1][0]
// ...
// 每个线程写入共享内存的不同位置！
```

**图解协作加载**（以TILE_WIDTH=4为例）：

```
线程块中有16个线程：
┌─────────────────────────────────────────────┐
│ 线程分布与共享内存写入位置：                 │
│ ┌───────┬───────┬───────┬───────┐           │
│ │(0,0)  │(0,1)  │(0,2)  │(0,3)  │           │
│ │写sA[0][0]│写sA[0][1]│写sA[0][2]│写sA[0][3]│ │
│ ├───────┼───────┼───────┼───────┤           │
│ │(1,0)  │(1,1)  │(1,2)  │(1,3)  │           │
│ │写sA[1][0]│写sA[1][1]│写sA[1][2]│写sA[1][3]│ │
│ ├───────┼───────┼───────┼───────┤           │
│ │(2,0)  │(2,1)  │(2,2)  │(2,3)  │           │
│ │写sA[2][0]│写sA[2][1]│写sA[2][2]│写sA[2][3]│ │
│ ├───────┼───────┼───────┼───────┤           │
│ │(3,0)  │(3,1)  │(3,2)  │(3,3)  │           │
│ │写sA[3][0]│写sA[3][1]│写sA[3][2]│写sA[3][3]│ │
│ └───────┴───────┴───────┴───────┘           │
└─────────────────────────────────────────────┘

结果是：所有线程协作填充了整个sharedA矩阵！
每个线程只写一个元素，但所有线程一起写满所有元素。
```

**简单理解就是：grid中有N/TN/T个block，block中有T*T个线程。每个block中的一个线程对应共享内存中一个元素。**

### 2.2 问题2：原代码的索引问题

**注意**：你提供的代码中有一个**关键的索引错误**：

```cpp
// 错误的索引：
int A_row_index = row;  // 这是错误的！

// 正确的应该是：
int A_row_index = by * TILE_WIDTH + ty;  // 而不是row
```

**为什么这是错的？**
因为所有在同一block中、`ty`相同但`tx`不同的线程，它们的`row`相同，但会尝试写入`sharedA[ty][tx]`的不同位置。这看似正确，但实际上：

1. 它们都是从A的**同一行**加载数据
2. 但`sharedA`应该存储A的一个**完整的平铺**（TILE_WIDTH行×TILE_WIDTH列）
3. 如果所有`ty`相同的线程都从A的同一行加载，那么`sharedA`的每一行都会被填成相同的值！

## 三、CUDA执行流程详细图解

### 3.1 硬件执行层次

```
物理硬件结构：
┌─────────────────────────────────────────┐
│ GPU设备                                  │
│  ┌───────────────────────────────────┐  │
│  │ 流式多处理器（SM）                 │  │
│  │  ┌─────────────┐ ┌─────────────┐  │  │
│  │  │ 核心        │ │ 核心        │  │  │
│  │  └─────────────┘ └─────────────┘  │  │
│  │  ┌─────────────────────────────┐  │  │
│  │  │ 共享内存（每个SM独立）       │  │  │
│  │  └─────────────────────────────┘  │  │
│  └───────────────────────────────────┘  │
└─────────────────────────────────────────┘
```

### 3.2 逻辑到物理的映射

```
启动配置：
<<<gridDim(2,2), blockDim(4,4)>>>

逻辑视图：
┌─────────────────────────────────────────┐
│ Grid (2×2个Blocks)                      │
│  ┌───────┬───────┐                      │
│  │ Block │ Block │                      │
│  │ (0,0) │ (0,1) │                      │
│  │ 4×4   │ 4×4   │                      │
│  ├───────┼───────┤                      │
│  │ Block │ Block │                      │
│  │ (1,0) │ (1,1) │                      │
│  │ 4×4   │ 4×4   │                      │
│  └───────┴───────┘                      │
└─────────────────────────────────────────┘

物理执行：
1. GPU调度器选择一个空闲的SM
2. 将一个Block分配给该SM
3. SM将Block中的线程分组为Warp（通常是32个线程一组）
4. Warp中的线程以SIMT方式执行（单指令多线程）
```

### 3.3 修正后的正确代码流程

```cpp
__global__ void matrixMulShared(float* A, float* B, float* C, int N) {
    __shared__ float sharedA[TILE_WIDTH][TILE_WIDTH];
    __shared__ float sharedB[TILE_WIDTH][TILE_WIDTH];

    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // 当前线程计算的C元素位置
    int row = by * TILE_WIDTH + ty;
    int col = bx * TILE_WIDTH + tx;

    float Cvalue = 0;

    // 循环所有平铺
    for (int t = 0; t < (N + TILE_WIDTH - 1) / TILE_WIDTH; ++t) {
        // === 协作加载A的平铺 ===
        // 注意：这里使用 by*TILE_WIDTH+ty 而不是 row
        // 因为我们要加载A的一个完整平铺，需要不同的行
        int A_row_idx = by * TILE_WIDTH + ty;    // 平铺中的行
        int A_col_idx = t * TILE_WIDTH + tx;     // 平铺中的列

        // === 协作加载B的平铺 ===
        int B_row_idx = t * TILE_WIDTH + ty;     // 平铺中的行
        int B_col_idx = bx * TILE_WIDTH + tx;    // 平铺中的列

        // 边界检查并加载
        if (A_row_idx < N && A_col_idx < N) {
            sharedA[ty][tx] = A[A_row_idx * N + A_col_idx];
        } else {
            sharedA[ty][tx] = 0.0;
        }

        if (B_row_idx < N && B_col_idx < N) {
            sharedB[ty][tx] = B[B_row_idx * N + B_col_idx];
        } else {
            sharedB[ty][tx] = 0.0;
        }

        // 等待所有线程完成加载
        __syncthreads();

        // === 计算部分和 ===
        for (int k = 0; k < TILE_WIDTH; ++k) {
            Cvalue += sharedA[ty][k] * sharedB[k][tx];
        }

        // 等待所有线程完成计算
        __syncthreads();
    }

    // 写回结果
    if (row < N && col < N) {
        C[row * N + col] = Cvalue;
    }
}
```

## 四、执行过程逐步图解

矩阵C每一个元素的结果由一个线程运算出来。分阶段运行一个阶段加载一次数据，计算一次，并累加计算结果。每个阶段最开始先不运算，只加载数据每个线程运行时填入一行一列A[i]B[j]的部分，然后等待全部线程加载完，完整的A[i]B[j]就在共享内存中。

### 4.1 阶段t=0的加载过程（以Block(0,0)为例）

```
假设：N=8, TILE_WIDTH=4
Block(0,0)有16个线程

加载A平铺（对应A[0:3, 0:3]）：
线程(ty,tx)  加载A元素       写入sharedA位置
(0,0)      A[0][0]  →  sharedA[0][0]
(0,1)      A[0][1]  →  sharedA[0][1]
(0,2)      A[0][2]  →  sharedA[0][2]
(0,3)      A[0][3]  →  sharedA[0][3]
(1,0)      A[1][0]  →  sharedA[1][0]
(1,1)      A[1][1]  →  sharedA[1][1]
(1,2)      A[1][2]  →  sharedA[1][2]
(1,3)      A[1][3]  →  sharedA[1][3]
(2,0)      A[2][0]  →  sharedA[2][0]
(2,1)      A[2][1]  →  sharedA[2][1]
(2,2)      A[2][2]  →  sharedA[2][2]
(2,3)      A[2][3]  →  sharedA[2][3]
(3,0)      A[3][0]  →  sharedA[3][0]
(3,1)      A[3][1]  →  sharedA[3][1]
(3,2)      A[3][2]  →  sharedA[3][2]
(3,3)      A[3][3]  →  sharedA[3][3]

加载B平铺（对应B[0:3, 0:3]）：
线程(ty,tx)  加载B元素       写入sharedB位置
(0,0)      B[0][0]  →  sharedB[0][0]
(0,1)      B[0][1]  →  sharedB[0][1]
(0,2)      B[0][2]  →  sharedB[0][2]
(0,3)      B[0][3]  →  sharedB[0][3]
(1,0)      B[1][0]  →  sharedB[1][0]
...（类似A的加载）
```

### 4.2 计算过程

```
每个线程的计算：
线程(ty,tx)计算：
Cvalue = Σ_{k=0}^{3} sharedA[ty][k] × sharedB[k][tx]

例如线程(1,2)：
Cvalue = sharedA[1][0]×sharedB[0][2] +
         sharedA[1][1]×sharedB[1][2] +
         sharedA[1][2]×sharedB[2][2] +
         sharedA[1][3]×sharedB[3][2]
```

## 五、Warp级别的执行细节

### 5.1 SIMT执行模型

```
Warp执行（32个线程一组）：
┌─────────────────────────────────────────┐
│ Warp中的32个线程同时执行相同的指令       │
│ 但操作不同的数据                        │
│                                         │
│ 时钟周期1：所有线程执行 sharedA[ty][tx]=...│
│ 时钟周期2：所有线程执行 if 语句          │
│ 时钟周期3：所有线程执行 __syncthreads() │
│ ...                                    │
└─────────────────────────────────────────┘
```

### 5.2 同步的重要性

```cpp
__syncthreads();  // 块内同步

为什么需要两次同步？
1. 第一次同步（加载后）：
   - 确保所有线程都完成了数据加载
   - 防止有的线程还在加载，有的已经开始计算

2. 第二次同步（计算后）：
   - 确保所有线程都完成了计算
   - 防止下次迭代覆盖共享内存时，还有线程在用旧数据
```

## 六、常见困惑解答

### Q1: 每个线程真的都执行所有代码吗？

**A**: 是的，但每个线程有自己的：

- 寄存器状态
- 局部变量（如`Cvalue`, `row`, `col`等）
- 程序计数器（跟踪执行位置）

### Q2: 共享内存是所有线程共享的吗？

**A**: 共享内存是**块内共享**：

- 同一个Block的所有线程共享同一块shared memory
- 不同Block的shared memory是独立的
- 线程只能访问自己Block的shared memory

### Q3: 线程如何知道自己该做什么？

**A**: 通过内置变量：

- `threadIdx.x/y/z`：知道自己在Block内的位置
- `blockIdx.x/y/z`：知道自己的Block在Grid内的位置
- 通过这些计算自己该处理的数据位置

## 七、可视化执行流程

```
时间轴视角：
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│  线程0      │ │  线程1      │ │  线程2      │
├─────────────┤ ├─────────────┤ ├─────────────┤
│ 计算row,col │ │ 计算row,col │ │ 计算row,col │
│ 初始化Cvalue│ │ 初始化Cvalue│ │ 初始化Cvalue│
│ 进入循环t=0 │ │ 进入循环t=0 │ │ 进入循环t=0 │
│ 计算A索引   │ │ 计算A索引   │ │ 计算A索引   │
│ 计算B索引   │ │ 计算B索引   │ │ 计算B索引   │
│ 加载到shared│ │ 加载到shared│ │ 加载到shared│
│ 同步点      │ │ 同步点      │ │ 同步点      │
│ 计算部分和  │ │ 计算部分和  │ │ 计算部分和  │
│ 同步点      │ │ 同步点      │ │ 同步点      │
│ 进入循环t=1 │ │ 进入循环t=1 │ │ 进入循环t=1 │
│ ...        │ │ ...        │ │ ...        │
└─────────────┘ └─────────────┘ └─────────────┘

注意：所有线程"同时"执行，但不是真正的并行
实际是SM以Warp为单位调度执行。
```

通过这样的详细解析，你应该能理解：

1. 每个线程都执行相同的代码，但处理不同的数据
2. 共享内存是通过所有线程协作填充的
3. 同步是确保正确协作的关键
4. 索引计算决定了每个线程处理哪部分数据
